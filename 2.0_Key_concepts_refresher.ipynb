{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2.0 Python concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Python Concepts Refresher\n",
    "\n",
    "As we start working with bigger text documents, sets of tokens and numerical representations, the amount of individual bits of information we need to store and refer back to becomes **massive**. We'll learn some new Python concepts to both handle large collections of data, and to process them.\n",
    "\n",
    "### Functions\n",
    "\n",
    "A function is a named block of code that performs a specific task. A number of thesse have been pre-programmed in Python, so to use them and run the code you can simply type the name of the function followed by () - inside which we can put the parameters we want it to use in performing the function. Here are the ones we have looked at already:\n",
    "\n",
    "#### `print()`\n",
    "\n",
    "The `print()` function in Python takes any what ever input you put inside the brackets and prints it out on one line of text on the screen, or other standard output device. It an take strings or other objects. \n",
    "\n",
    "#### `split()`\n",
    "\n",
    "The `split()` function in Python can be used to turn a sentence into a list of words\n",
    "\n",
    "```\n",
    "text = \"this is my text and it contains words\"\n",
    "print(text.split())\n",
    "```\n",
    "\n",
    "#### `len()`\n",
    "\n",
    "The `len()` function in Python can be used to get the **length of a given string**. \n",
    "\n",
    "```\n",
    "letters_in_book = len(book_text)\n",
    "```\n",
    "\n",
    "#### `Indexing`\n",
    "\n",
    "We can also extract **specific characters** from a string, given a **numerical index**. \n",
    "\n",
    "We do this using ``square brackets[]``. \n",
    "\n",
    "In coding, we **start counting at 0**. This means the first letter of a string is as follows \n",
    "\n",
    "``\n",
    "first_letter_in_book = book_text[0]\n",
    "``\n",
    "\n",
    "It also means **last character** will be at index **length - 1**. So we can also combine with the `len()` function to calculate this!\n",
    "\n",
    "\n",
    "#### `Slicing`\n",
    "\n",
    "If we want to get **more than 1 character**, we can provide two indexes with a ``colon:`` inbetween to specify a range.\n",
    "\n",
    "```\n",
    "first_five_letters = book_text[0:5]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "If it's the first time you are using a library you may also need to install it. \n",
    "\n",
    "You can do this using pip in the following way. \n",
    "\n",
    "`pip install package_name`\n",
    "\n",
    "\n",
    "If you are working in Colab you need to add an exclamation mark in front. For example, to install numpy:\n",
    "\n",
    "`!pip install numpy`\n",
    "\n",
    "You can also search for and add libraries or 'packages' within the Anaconda GUI (graphical user interface) console.\n",
    "\n",
    "\n",
    "\n",
    "## import \n",
    "\n",
    "Once we have installed them on our computer, we need to tell the code that we want them availabke to use. We do this using `import`. \n",
    "\n",
    "We can 'import' existing libraries - or packages of code - that are widely available.\n",
    "\n",
    "Some that will be useful are:\n",
    "\n",
    "- nltk,  is especially for working with textual data, and has a lot of inbuilt functions to perform key tasks.\n",
    "(Check out the NLTK book: https://www.nltk.org/book/ch01.html)\n",
    "- spaCy, is another a free and open-source Natural Language Processing (NLP) package. If you're interested in learning how to work with spaCy more broadly for a variety of NLP tasks I recommend the tutorial Natural Language Processing with spaCy in Python: https://realpython.com/natural-language-processing-spacy-python/.\n",
    "- gensim, is dedicated to topic modeling, and has some really useful tutorials and materials to read through: https://radimrehurek.com/gensim/auto_examples/index.html#core-tutorials-new-users-start-here  \n",
    "- pandas, a data analysis library\n",
    "- numpy, a mathematical functions library\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Documents as Numbers\n",
    "## New concepts for week 2 \n",
    "\n",
    "\n",
    "If we want to find take a document or set of documents, and use machine learning techniques, or other mathematical operations to uncover some new information abou them, we can't just use the text and characters. We need a new representation, and that need to be numerical. This week we will be taking our first steps to representing collections of documents as numbers. \n",
    "\n",
    "- Tokenisation \n",
    "- Bags of Words \n",
    "- n-Grams\n",
    "- TF/IDF\n",
    "\n",
    "### Building a Vocabulary \n",
    "\n",
    "The first step to getting a new, better representation of a text document is splitting it up into its consistuent parts. We call these **tokens** and deciding _what a token is_ in important choice. \n",
    "\n",
    "### Basic Tokenisation - ``str.split()``\n",
    "What is the simplest way split a **String** into **tokens**? \n",
    "\n",
    "Introducing the `str.split()` function. Here we take a long string (multiple words) and split it into separate words (or **tokens**) based on spaces. \n",
    "\n",
    "We have previously seen **Functions** that take a String as an argument and return a new value. In this case, the concept is broadly similar, apart from we **call the function on the string itself**.\n",
    "\n",
    "What gets returned is a **List**, containing our split string. We store it in the variable ``tokens``, and print it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the sentence below\n",
    "\n",
    "sentence = \"I'm learning new things every day\"\n",
    "tokens = sentence.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably won't understand all of the code in the following parts, but thats fine. You're Great. \n",
    "\n",
    "We've picked up some super important new concepts that we'll keep practising throughout the course. \n",
    "\n",
    "Now, we'll learn some new concepts for analysing text\n",
    "\n",
    "### How good is our vocabulary?\n",
    "\n",
    "We have made a vocabulary from, (or **tokenised**, if you're fancy) our text document by splitting every time we see a space.\n",
    "\n",
    "Does this seem sensible? Does this capture every thing that we would consider a separate word in the document? \n",
    "\n",
    "What about `isn't`? Is this one token (`isn't`), or two tokens (`is` and `not`)? Or `taxi cab`? Is that two tokens, do we care that `taxi` and `cab` are both used? Do we need this as a separate concept from `Uber`, `limo`, `Hackney Carriage` or `car`?. If we take it as one, do we miss out on other combinations like `black cab` or `taxi driver`? What about punctuation?\n",
    "\n",
    "Just using `str.split()` works reasonably well on the sentence below. However, there are some issue with punctuation as ideally, the brackets and the exclaimation mark would also be separate tokens. \n",
    "\n",
    "After we have split our sentence into tokens, we can then create a **vocabulary**, which contains every unique token in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the numpy library, name it np for shorthand so that we can refer to it in the code as 'np'\n",
    "\n",
    "This allows us to get unique words in our vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#String split\n",
    "sentence = \"I like to think (it has to be!) of a cybernetic ecology where we are free of our labors\"\n",
    "tokenised = sentence.split()\n",
    "print(\"Tokenised sentence\")\n",
    "print(tokenised)\n",
    "\n",
    "#Get the unique tokens (removes duplicates)\n",
    "vocab = np.unique(tokenised)\n",
    "print(\"Vocabulary\")\n",
    "print(vocab)\n",
    "print(\"unique words\", vocab.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import \n",
    "\n",
    "We can 'import' existing libraries - or packages of code - that are widely available.\n",
    "\n",
    "Some that will be useful are:\n",
    "\n",
    "- nltk,  is especially for working with textual data, and has a lot of inbuilt functions to perform key tasks.\n",
    "(Check out the NLTK book: https://www.nltk.org/book/ch01.html)\n",
    "- spaCy, is another a free and open-source Natural Language Processing (NLP) package. If you're interested in learning how to work with spaCy more broadly for a variety of NLP tasks I recommend the tutorial Natural Language Processing with spaCy in Python: https://realpython.com/natural-language-processing-spacy-python/.\n",
    "- gensim, is dedicated to topic modeling, and has some really useful tutorials and materials to read through: https://radimrehurek.com/gensim/auto_examples/index.html#core-tutorials-new-users-start-here  \n",
    "- pandas, a data analysis library\n",
    "- numpy, a mathematical functions library\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Tool Kit (NLTK)\n",
    "\n",
    "NLTK is one of many libraries - or packages of code that are available -  is especially for working with textual data. It has in built functions for working with text.\n",
    "\n",
    "Check out the NLTK book:\n",
    "https://www.nltk.org/book/ch01.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB. Importing libraries\n",
    "\n",
    "To use a library you need to tell the program that you need it. ...\n",
    "\n",
    "Sometimes you will need to install the library if it is not already on your computer.\n",
    "\n",
    "The best way to install libraries is using pip - this will work if you are working with either colab or anaconda..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the comment hashtags to tell the code to run\n",
    "\n",
    "# pip install nltk\n",
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has functions inbuilt to perform many of the tasks you need.\n",
    "\n",
    "`sent_tokenize()` - splits your text into sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "EXAMPLE_TEXT = \"NLTK is one of many libraries. Or packages of code that are available. Do you think the Natural Language Toolkit is especially useful for working with textual and linguistic data\"\n",
    "\n",
    "\n",
    "sent_tokenize(EXAMPLE_TEXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add your own example sentence to see how it handles contractions such as don't / isn't. \n",
    "\n",
    "EXAMPLE_TEXT = \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Concepts - Lists\n",
    "\n",
    "As we start working with bigger text documents, sets of tokens and numerical representations, the amount of individual bits of information we need to store and refer back to becomes **massive**. We'll learn some new Python concepts to both handle large collections of data, and to process them.\n",
    "\n",
    "### Lists \n",
    "\n",
    "Previously, we'd used **named variables** to store individual bits of information such as text and numbers. \n",
    "\n",
    "```\n",
    "my_diary_entry = \"Today I mainly SMASHED IT\"\n",
    "hours_spent_smashing it = 1000\n",
    "```\n",
    "\n",
    "But if we look at the ``split()`` function we have just used, it returns **6 different values**. And it would seem like a lot of effort to have a named variable for each of them?\n",
    "\n",
    "``\n",
    "token_1 = \"I'm\"\n",
    "token_2 = \"learning\"\n",
    "token_3 = \"new\"\n",
    "token_4 = \"things\"\n",
    "token_5 = \"every\"\n",
    "token_6 = \"day\"\n",
    "``\n",
    "\n",
    "And what happens when we have 1000 tokens? Or 30,000 (this is the size of the average english speakers vocabulary)?\n",
    "\n",
    "Instead, we can store collections of values in a **single object** known as a ``List``. You may also here the terms ``array`` and ``vector``, and whilst they do mean specific things in specific circumstances, these are all broadly interchangeable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lists\n",
    "\n",
    "sentences = []\n",
    "sentences = sent_tokenize(EXAMPLE_TEXT)\n",
    "words = word_tokenize(EXAMPLE_TEXT)\n",
    "print(sentences[1])\n",
    "print(words[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with your own text data \n",
    "\n",
    "Look at the ipython notebook 2.1 to see how to load in a file (or move over to 2.2 where this is already implemented, or simply add a paragraphy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sone text to work with. Either load a text file or you can just copy and past a paragraph below. \n",
    "\n",
    "\n",
    "# EXAMPLE_TEXT = \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing steps\n",
    "\n",
    "Tokenizing is the first step we take to process our data to prepare it for converting it to a numerical representation. There are other steos we might take too..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words\n",
    "\n",
    "Before we start counting words, we might want to consider which words we are interested in counting.\n",
    "\n",
    "Some words are frequent but don't carry much meaning in ad of themselves, for examples common words in English such as \"a\", \"the\", \"it\".  \n",
    "\n",
    "NLTK has a pre-defined set of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#This regular expression splits based on space AND punctuation\n",
    "word_tokens = re.split(r'[-\\s.,;!?]+', EXAMPLE_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can filter our text to remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = []\n",
    "\n",
    "filtered_text = [w for w in word_tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokens)\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Frequency Distribution\n",
    "\n",
    "We can count the frequency of each unique word in the text to create a frequency distribution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter is a module that helps with counting\n",
    "from collections import Counter\n",
    "\n",
    "# Count the frequency of words\n",
    "word_freq = Counter(filtered_text)\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the ten most common words\n",
    "\n",
    "common_words = word_freq.most_common(10)\n",
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the plot inline in the notebook with interactive controls\n",
    "# Comment out this line if you are running the notebook in Deepnote\n",
    "%matplotlib notebook\n",
    "\n",
    "# Import the matplotlib plot function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a list of the most common words\n",
    "words = [word for word,_ in common_words]\n",
    "\n",
    "# Get a list of the frequency counts for these words\n",
    "freqs = [count for _,count in common_words]\n",
    "\n",
    "# Set titles, labels, ticks and gridlines\n",
    "plt.title(\"Top 10 Words in my text\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(range(len(words)), [str(s) for s in words], rotation=90)\n",
    "plt.grid(visible=True, which='major', color='#333333', linestyle='--', alpha=0.2)\n",
    "\n",
    "# Plot the frequency counts\n",
    "plt.plot(freqs)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Pre-processing steps\n",
    "\n",
    "\n",
    "The goal of both stemming and lemmatization is to collapse related forms of a word to a common base form. \n",
    "\n",
    "For instance:\n",
    "\n",
    "am, are, is $\\Rightarrow$ be\n",
    "\n",
    "car, cars, car's, cars' $\\Rightarrow$ car \n",
    "\n",
    "## Stemming\n",
    "\n",
    "Stemming attempt to remove suffixes from words that contain the same base. This reduces variation and can help when we reduce the documents into a more distilled form (like a bag of words). \n",
    "\n",
    "- hacking, hackers, hacked, hacks\n",
    "- computer, computing, computers\n",
    "\n",
    "Depending on our task, its probably the case that we only really care about knowing if **any** of these words appear, not whether they each appear individually. For example, I might be doing a search for paragraphs about hacking and it may be that I would miss out on key documents otherwise if I only searched for one of the words. \n",
    "\n",
    "Stemming can be quite a challenging task however. If we want to combine pluralisations, for example, we can't just remove the \"s\" from the end of all nouns, what about \n",
    "\n",
    "- grass (not a plural)\n",
    "- mice, octopi (plural, no s)\n",
    "- geniuses (plural, es)\n",
    "\n",
    "We're going to use the built in stemmer in the NLTK library. This reduces our vocabulary in the hacking book dramatically!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"It is important to be very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "word_list = ['feet', 'foot', 'foots', 'footing']\n",
    "for word in word_list:\n",
    "    print(word, \"->\", stemmer.stem(word))\n",
    "#Doesn't always work\n",
    "word_list = ['organise','organises','organised','organisation',\"organs\",\"organ\",\"organic\"]\n",
    "for word in word_list:\n",
    "    print(word, \"->\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematization\n",
    "Lemmatisation is a technique similar to stemming, apart from it attempts to find similar meanings, as opposed to just similar roots. Like with all these _normalisation_ techniques, reducing your vocabulary will reduce precision but may make your model bettter at generalising and more efficient.\n",
    "\n",
    "For example, lemmatisation would be able to separate **dogged** and **dog**, which have quite different meanings but would get combined by a stemmer. \n",
    "\n",
    "Below we use the WordNetLemmatizer from the NLTK library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outouts of stemmer and lemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer \n",
    "stemmer = PorterStemmer() \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "print(stemmer.stem('stones')) \n",
    "print(stemmer.stem('speaking')) \n",
    "print(stemmer.stem('bedroom')) \n",
    "print(stemmer.stem('jokes')) \n",
    "print(stemmer.stem('lisa')) \n",
    "print(stemmer.stem('purple')) \n",
    "print('----------------------') \n",
    "print(lemmatizer.lemmatize('stones')) \n",
    "print(lemmatizer.lemmatize('speaking'))\n",
    "print(lemmatizer.lemmatize('bedroom'))\n",
    "print(lemmatizer.lemmatize('jokes'))\n",
    "print(lemmatizer.lemmatize('lisa'))\n",
    "print(lemmatizer.lemmatize('purple'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet\n",
    "\n",
    "Wordnet, a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"small\"):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept.\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets(\"pain\")\n",
    "print(syn[0].definition())\n",
    "print(syn[0].examples())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capitalisation\n",
    "Whilst it may be tempting to just lower case every token with the belief that words all have the same meaning regardless of case. However, it may actually be that if something is in ALL CAPS it conveys some meaning. Or if a word is at the start of sentence, that has importance. \n",
    "\n",
    "For example \n",
    "\n",
    "- John liked to help\n",
    "- John screamed HELP HELP HELP\n",
    "\n",
    "Or if one book contained lots of capitalised nouns (like cities and countries), it might tell you it was about Geography.\n",
    "\n",
    "Some libraries actually account for this by lower casing everything, then having a token which indicates a start of capitilising as well as one that signifies the end of capitalising. This allows the best of both worlds. Of course, this only works if your model or vocabulary is able to take sequence and context into account. Like for example...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go Further\n",
    "\n",
    "If you would like something more challenging check out these notebooks which includes topic modelling on a shakespeare corpus:\n",
    "\n",
    "https://github.com/sgsinclair/alta/blob/master/ipynb/ArtOfLiteraryTextAnalysis.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
