{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2.2 code for textual analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "If it's the first time you are using a library you may also need to install it. \n",
    "\n",
    "You can do this using pip in the following way. \n",
    "\n",
    "`pip install package_name`\n",
    "\n",
    "\n",
    "If you are working in Colab you need to add an exclamation mark in front. For example, to install numpy:\n",
    "\n",
    "`!pip install numpy`\n",
    "\n",
    "You can also search for and add libraries or 'packages' within the Anaconda GUI (graphical user interface) console.\n",
    "\n",
    "\n",
    "\n",
    "## import \n",
    "\n",
    "Once we have installed them on our computer, we need to tell the code that we want them availabke to use. We do this using `import`. \n",
    "\n",
    "We can 'import' existing libraries - or packages of code - that are widely available.\n",
    "\n",
    "Some that will be useful are:\n",
    "\n",
    "- nltk,  is especially for working with textual data, and has a lot of inbuilt functions to perform key tasks.\n",
    "(Check out the NLTK book: https://www.nltk.org/book/ch01.html)\n",
    "- spaCy, is another a free and open-source Natural Language Processing (NLP) package. If you're interested in learning how to work with spaCy more broadly for a variety of NLP tasks I recommend the tutorial Natural Language Processing with spaCy in Python: https://realpython.com/natural-language-processing-spacy-python/.\n",
    "- gensim, is dedicated to topic modeling, and has some really useful tutorials and materials to read through: https://radimrehurek.com/gensim/auto_examples/index.html#core-tutorials-new-users-start-here  \n",
    "- pandas, a data analysis library\n",
    "- numpy, a mathematical functions library\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy as np\n",
    "\n",
    "Import the numpy library, name it np for shorthand so that we can refer to it in the code as 'np'\n",
    "\n",
    "This allows us to get unique words in our vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, os\n",
    "import csv\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupter notebook / Ananconda users\n",
    "Save files that you want to use in the same folder as the ipython notebook you are running (or include a full file path, e.g. rather than `f = open('frankenstein.txt')` something like `f = open('Location/MyFolder/frankenstein.txt')`\n",
    "\n",
    "\n",
    "# Colab users\n",
    "\n",
    " You will need to add any files you wish to use by clicking on the file icon on the left of the screen and uploading the txt file there. Click on the file with an arrow on it, then select the file and it will be added to the file list,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your text \n",
    "f = open('frankenstein.txt')\n",
    "text = f.read()\n",
    "\n",
    "# if you would rather copy and paste text like we did in the tutorial, uncomment the code below and add your own text\n",
    "\n",
    "# text = \" add your text here \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will make all words lowercase so that *that* and *That* \n",
    "# will be counted together when we come to count words!\n",
    "\n",
    "\n",
    "text = text.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "\n",
    "NLTK has functions inbuilt to perform many of the tasks you need.\n",
    "\n",
    "`sent_tokenize()` - splits your text into sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "## tokenize the data and store the tokens in a lists\n",
    "\n",
    "sentences = []\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(sentences[1])\n",
    "\n",
    "print(tokens[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique tokens and basic counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the unique tokens (our vocabulary)\n",
    "vocab = np.unique(tokens)\n",
    "print(\"total words:\",len(tokens), \"unique words:\", len(vocab))\n",
    "#Create a Bag of Words using a Counter\n",
    "Counter(tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has counted all of the punctuation as tokens, we might not want them included..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation \n",
    "There are lots of different ways to remove punctuation, for example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "test_sample = \"This is a sentence - it has lots, that's right, lots of punctuation!!!!\"\n",
    "test_sample_no_punct = tokenizer.tokenize(test_sample)\n",
    "\n",
    "print(test_sample)\n",
    "print(test_sample_no_punct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also do it by writing our own regular expression or regex\n",
    "# A regular expression is a sequence of characters that specifies a search pattern in text. \n",
    "# Read more about Regex here: https://www.w3schools.com/python/python_regex.asp \n",
    "\n",
    "#So instead let's just u a regex to split based on space AND punctuation\n",
    "tokens = re.split(r'[-\\s.,;!?]+', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this time when we run this piece of code the punctuation should have gone\n",
    "\n",
    "vocab = np.unique(tokens)\n",
    "print(\"unique words\", vocab.shape)\n",
    "Counter(tokens).most_common(50)\n",
    "\n",
    "#Get the unique tokens (our vocabulary)\n",
    "vocab = np.unique(tokens)\n",
    "print(\"total words:\",len(tokens), \"unique words:\", len(vocab))\n",
    "#Create a Bag of Words using a Counter\n",
    "Counter(tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the punctuation is gone there are still lots of words which aren't the most informative - like 'are', 'at', etc. \n",
    "\n",
    "Also can you spot the duplicates? Why might that be happening? We have *the* and *The*....\n",
    "\n",
    "We need to make all words lowercase so that *the* and *The* are counted together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words\n",
    "\n",
    "Before we start counting words, we might want to consider which words we are interested in counting.\n",
    "\n",
    "Some words are frequent but don't carry much meaning in ad of themselves, for examples common words in English such as \"a\", \"the\", \"it\".  \n",
    "\n",
    "NLTK has a pre-defined set of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you want to add additional stopwords you would do it like this...\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "# newStopWords = ['pick','some', 'words','to','add']\n",
    "# stopwords.extend(newStopWords)\n",
    "# print(stop_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can filter our text to remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = []\n",
    "\n",
    "filtered_tokens = [w for w in tokens if not w in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the effect this has on the text by runing teh next two bloack of code (if you are using a text other thank frakenstein the slices selected won't make sense so you can swal the numbers to `[0:100]` in both boxes for a more straightforward comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens[280:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_tokens[194:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now stop words and punctuation gace been removed let's plot our frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter is a module that helps with counting\n",
    "from collections import Counter\n",
    "\n",
    "# Count the frequency of words\n",
    "word_freq = Counter(filtered_tokens)\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Frequency Distribution\n",
    "\n",
    "We can count the frequency of each unique word in the text to create a frequency distribution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter is a module that helps with counting\n",
    "from collections import Counter\n",
    "\n",
    "# Count the frequency of words\n",
    "word_freq = Counter(filtered_tokens)\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the ten most common words\n",
    "common_words = word_freq.most_common(10)\n",
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the plot inline in the notebook with interactive controls\n",
    "# Comment out this line if you are running the notebook in Deepnote\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "# Uncomment the following line if you are in Colab\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the matplotlib plot function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a list of the most common words\n",
    "words = [word for word,_ in common_words]\n",
    "\n",
    "# Get a list of the frequency counts for these words\n",
    "freqs = [count for _,count in common_words]\n",
    "\n",
    "# Set titles, labels, ticks and gridlines\n",
    "plt.title(\"Top 10 Words in my text\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(range(len(words)), [str(s) for s in words], rotation=90)\n",
    "plt.grid(visible=True, which='major', color='#333333', linestyle='--', alpha=0.2)\n",
    "\n",
    "# Plot the frequency counts\n",
    "plt.plot(freqs)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go Further\n",
    "\n",
    "If you would like something more challenging check out these notebooks which includes topic modelling on a shakespeare corpus:\n",
    "\n",
    "https://github.com/sgsinclair/alta/blob/master/ipynb/ArtOfLiteraryTextAnalysis.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
